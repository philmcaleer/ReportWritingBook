# General Writing

### Is there one way to write a full report/paper?

No. A lot of materials on writing reports will tell you to do this or to do that. In the vast majority of cases this advice will be nothing more than that, advice, not hard and fast rules. That said, that advice is often given based on what makes a better reading experience for your intended audience and so is worth paying attention to. There will of course be some rules, on aspects such as presentation of information, values, citations, etc and, given that we use APA 7th edition, it is worth referring to this webpage and the instructional aids there for pointers on that aspect: [https://apastyle.apa.org/style-grammar-guidelines](https://apastyle.apa.org/style-grammar-guidelines){target="_blank"}

### What is the best way to write a report?

Again no one real answer but one key thing to keep in mind is your reader. Often we fall into the trap of thinking solely as the writer, because we are writing the report, and forget about the reader. So one good bit of advice is to write like a writer and redraft like a reader. This means that when you are redrafting your work, take a perspective of a reader who knows about psychology but perhaps does not know this topic so well. This will help you spot where ideas and information are not as clear as they could be - there is some evidence to suggest that reading your work out loud can help spot issues when redrafting as you, the writer, are having to slow down to read it, to a pace similar to the novel reader: ["When redrafting read out loud"](https://www.learningscientists.org/blog/2022/10/20-1). We also recommend this student guide to scientific writing by [Carpenter (2020)](https://psyarxiv.com/r4sfz/) for some principles and skills to work on. 

## When is the deadline again and what are the marking criteria?

Best to check the Assessment Information Sheet on Moodle. We believe it is better to have one document that has all the correct information rather than multiple documents having conflicting information so we will keep that answer to the AIS.

### Is there a recommended word count for each part of the report?

So in terms of word count, whilst there is no specific hard and fast word count for each section a good ballpark is something like a third, a third, a third. Third on introduction, third on discussion, third on the methods, results, abstract.  Don't take that verbatim though and see it just as a ballpark. We would suggest that the introduction and discussion should balance (meaning you should try to avoid having a very long introduction and a very small discussion, or vice versa), and any words we can save from the Methods and Results, we would use that space in the Introduction and Discussion to add more depth or support. Methods and Results are formulaic and detailed, they don't need a lot of expansion - so if you can get the detail across in less words then those spare words we would put into the introduction and discussion. The introduction and discussion of a paper do a lot of the heavy lifting basically.

### Do we need to include an Abstract?

Yes. Estimate somewhere around 250 words for the abstract. In reality most abstracts will be around 200-250 words and that allows more words for other parts. Main thing is that the Abstract covers the key parts of Area, Aim, Methods, Results, Conclusion.

### What is included in the word count?

An easier way is to say what is not included in the word count. The title, the references, and the appendices (if you have any) are the only things not included in the word count. Everything else - including figure legends, table legends, values in tables, citations, etc, and all the text from the abstract to the end of the discussion, are included in the word count

### Can we use abbreviations?

Abbreviations are really not great for readers - they are only good for writers in that it helps them save space. For a reader it creates a high cognitive demand as they try to remember what the abbreviation stood for, ultimately distracting them from reading and understanding what you are writing - an example here might be using SE for Self-Efficacy and IM for Intrinsic Motivation. Once you have written your paper these abbreviations might be very clear to you, but taking this approach makes the writing very hard to understand for the reader, and we would discourage it at all times. The exception here is questionnaires (e.g. MSLQ) and well-known techniques or methods (e.g. MRI, MEG, EEG) but remember to state the phrase out in full in the first instance.

### Can we use quotations from previous work?

Technically this is allowed but would be better if you didn't and just rephrased the writing in your own words. For one, quotations change the tone of the writing from your tone to that of the other writer. More importantly, when you use a quotation, and don't go into depth about the meaning of that quotation, you are relying on the reader getting the exact same meaning from the quotation that you did but they are only getting the quotation and are not getting any of the context. A much better approach is to restate the meaning of what you want to say so that it fits and flows with the surrounding context of your writing.

### Can we use do not or don't?

Do not use don't and other such shortenings like can't and shouldn't. Instead use do not etc. Remember academic tone! Contractions, as they are called, are fine for informal writing in lay summaries, blogs and webpages. For an academic report though they are considered bad practice and should be avoided.

### Can we use footnotes?

We would say no. They are not the done thing in academic reports in Psychology so best avoided. Again they are not conducive to an easy reading experience of the reader - the reader has to move their eyes from where they were reading to the bottom of the page to find the footnote and then back to where they were reading in the page. This prevents a flow of information in your writing and so are best avoided.

### Can we write 'I' or 'we' in our report?

Yes. In this specific report you could use “We” because it started as a group effort or you could use "I" as ultimately it is you doing the analysis and writing. More seasoned (a nice word for older) members of the team find it easier to write "We" as science is rarely just a singular person doing something but both approaches are fine here. We would however avoid using "The research team" and just state "we" - active voice is easier to read than passive voice. 


### Can you use subheadings in an Introduction and a Discussion?

Yes. There is no rule against this. The question is really do you need them? Often when people use subheadings in an introduction and discussion it leads them to write separate sections in the introduction and the different parts are not connected or joined in any way and it reads like different reports in the one introduction. If you do use subheadings try to think about your writing and aim to still connect the different sections and show they are interlinked in some fashion. Even a simple sentence like "A second important question related to Self-Efficacy is..." This at least shows some brief acknowledgement of the first section and that everything is tied to self-efficacy and questions around it. Do remember though that subheadings count in the word count and if your sections are interlinked then you probably don't need them.


### Is there an appropriate number of citations to use in a report?

It is a tough thing to answer as there is no right answer but you basically want to watch out for too few and too many citations. Particularly when you have a word count. You need to be careful of giving a whole lot of citations (5, 6, 7, etc) to support a point when two or three would have done. At one point you start taking away words that you could have used to clarify the point better for the reader. So that is really the thing you are trying to do - make sure a point is clear and is supported. Some people aim for about two or three citations to show a point is supported, but that is really rough and sometimes it might be 4 or it might be 1 depending on how well supported the point actually is. If only one paper has looked at something then I can only cite one paper. If it helps, some journals recommend no more than 40 citations for a 3000-4000 article and you are probably not too far wrong if you aim for something like that.

### Do you use citations differently in different parts of the introduction?

Whilst there is no rule here, some tend to be a bit heavy on citations in the opening paragraph of an introduction, and here you might see the citations per point creep up a little as people set the scene and just try to give a broad overview. After that first paragraph the tendency is to only use numerous citations where needed. Really, making sure the detail comes across - whatever detail you need your reader to focus on - is better than 4 or 5 additional citations beyond the two or three that already showed the point. The distinction here is that, at the start, you are less focused on details and more focused on general findings, so you add a few additional citations maybe, whereas the further you get into the introduction the more detail you tend to pull out, so the citations get a little less heavy and a lot more focused. In addition, from a purely blunt word count perspective, one way to think about it is that if we say on average a citation is four words - citation et al., 20XX - if three citations show the point but you add 7 citations to really hammer it home, then those 4 additional citations use up 16 words that could have been used elsewhere. Ultimately it is always a trade-off sadly but hopefully that gives some insight!

### Can you summarise the issues of too few and too many citations?

Again this is trying to put a number on something that doesn't really have a rule in the world. The main thing to think about is showing support for points and showing relevant detail to help the reader understand the point you are making. You are the expert and your role is to convey information to a reader efficiently. But to try to give two points to watch out for:

* Using one citation or repeatedly using the same citation suggests a limited background to the research and doesn't give the reader a good sense of what is already know. All the information is taken from one perspective. Imagine hearing about an incident or an event where you only here it from one person. They might give you lots of detail but you can't verify it is supported by others.
* Using lots and lots of citations may lead to running out of words and whilst the reader sees the support for the point, the point you are making, in itself, might not be clear and may be lacking the detail that the reader needs to understand what is being talked about. Imagine hearing about an incident or an event from lots and lots of people. In this instance you get to know that a lot of people are talking about the incident but you probably can't take in all the details or the details are short as you are trying to hear from everyone.

So the trade-off of detail and support is the balance you are aiming for when thinking about how many citations to include.



### What is Critical Evaluation?

This paragraph below has had all the points of critical evaluation highlighted in bold.

"In regard to perceived trustworthiness, **as with dominance in females, few** studies have specifically targeted the effect of vocal pitch on this trait, **with results proving largely inconsistent.** In male voices, two studies exploring the effect of vocal pitch on election voting and personality traits **found contrasting results;** Tigue et al. (2012) found that low-pitched candidates were perceived as more trustworthy, **whereas** Klofstad et al. (2012) found no significant preference in pitch with regard to trustworthiness. **Similarly,** exploring the effect of seeking short- versus long-term relationships, Vukovic et al. (2011) showed no significant preference in male pitch when female participants judged the voices for trustworthiness. **However,** McAleer et al. (2014) found that high-pitched male voices were rated as more trustworthy compared with low-pitched voices. Considering trustworthiness in female voices, **as is the case with dominance, there is a lack of studies, and again a lack of consistent findings.** Klofstad et al. (2012) found that low-pitched female speakers were judged as being more trustworthy in a political scenario, **while, in contrast,** McAleer et al. (2014) found that trustworthiness in female voices was not influenced by average voice pitch but by the vocal glide and intonation; how the pitch moves. **In short, in both genders, there is a lack of studies examining the role of pitch and the preference of high versus low pitch in trustworthiness judgments, with previous findings having failed to provide a consistent answer.**"

If we now look at this version of the paragraph where the critical evaluation has been removed but communication and research has, to some degree, been maintained:

"In regard to perceived trustworthiness studies have looked at the effect of vocal pitch on this trait. In male voices, two studies exploring the effect of vocal pitch on election voting and personality traits are Tigue et al. (2012) who found that low-pitched candidates were perceived as more trustworthy, and Klofstad et al. (2012) who found no significant preference in pitch with regard to trustworthiness. Looking at the effect of seeking short- versus long-term relationships, Vukovic et al. (2011) showed no significant preference in male pitch when female participants judged the voices for trustworthiness. McAleer et al. (2014) found that high-pitched male voices were rated as more trustworthy compared with low-pitched voices. Considering trustworthiness in female voices, Klofstad et al. (2012) found that low-pitched female speakers were judged as being more trustworthy in a political scenario. McAleer et al. (2014) found that trustworthiness in female voices was not influenced by vocal glide and intonation; how the pitch moves."

Hopefully what you can see, in the first version, is that the critical evaluation parts are turning the paragraph from a purely descriptive list of findings, in the second version, into a discussion about how previous findings relate to each other (how they compare, align, contrast), and how they all come together in your report to help build the argument/rationale for your work (or the rationale for your theory). This paragraph comes from [Tsantani et al., 2016](https://journals.sagepub.com/doi/10.1177/0301006616643675){target="_blank"}. Our thanks to Eniko Dobos for help in creating this example.

### Do you have any guidance for writing paragraphs?

This blog is one that we quite like: [How to write paragraphs](https://blogs.lse.ac.uk/writingforresearch/2017/07/17/how-to-write-paragraphs-in-research-texts-articles-books-and-phds/){target="_blank"}. We do not agree with everything it says, in particular point 5 that a paragraph can't be more than 250 words, but a lot of the other points to watch out for are really helpful, and the general overall principle of the structure of a paragraph is helpful as well - you may also know it as the Point, Evidence, Explain, Link approach we looked at earlier in the semester.

## Using the Pre-Registration

### Should we include the pre-registration in the submission?

This is not essential but normally we ask that you include your answers to the first 5 questions from the pre-registration as an appendix to the full report. Do not worry if you forget this though as it is not essential and just recommended. You can include the code as well if that is easier but again not essential. Having the pre-registration as an appendix helps the reader check things if they are uncertain.

### Can we refer to the pre-registration in the appendix?

Yes but this must not be used as a means to save words. For example, you can say "We excluded participants based on the criteria of under 21, male and mature student (see Appendix A)" but you can't say "We excluded participants based on criteria (see Appendix A)". The report has to stand alone and as such **all relevant information should be contained in the full report, not just the Pre-Registration**.

### Can we use points, citations, research raised in the pre-registration for the full report?

Absolutely. You should definitely use the same research and citations, at least to start off from. You can also make the same points though just watch the wording and trying to make sure that the phrasing is in your own tone and in your own words. In short, the pre-registration is a snapshot of what you are doing. The full report is a deeper expansion on those ideas.

### Can we or should we compare the results from our full dataset to the results from the pre-registration?

No! Really no! The pre-registration is just a template to help you think about what you want to do with your data. The results from the pre-registration should not be talked about in the full report. 

### Can I change my pre-registration to avoid having to write about a deviation from the pre-registration?

The real question is "should you change your pre-registration?" The answer to that would be no, we would probably suggest just leaving the pre-registration as is and writing about the deviation briefly as we showed in the Results section. To do good and honest science involves having to write up some of the issues we encounter. That is fine!

### I have now done a lot of reading and think my pre-registration is completely wrong and want to change it all. Can I?

Ultimately we will grade and give feedback on the full report you submit. It is unadvised to change the pre-registration because you won't have any feedback on it, and your grade for the pre-registration won't change, but ultimately we will grade and give feedback on the full report you submit and not on the pre-registration.

# Introductions & Discussions

### What’s the difference between an introduction and a literature review?

The purpose of an Introduction is to provide a rationale for the study that you are going to conduct. It will contain elements of a literature review because you will discuss previous work as part of building this rationale, however, you also need to try and identify and highlight the reason why you conducted the study you did so you will not need every detail of every previous research. A literature review generally tells a much larger fuller story of a field; an Introduction is only part of the story of that field. 

### What is the best way to structure our introduction?

The most common advice is to run broad to narrow and that holds here as well. The tricky part is that you are working with probably two variables and trying to bring that altogether. Obviously there are a million different ways to bring all this information together but if it was us we would default to the notion of no, matter what we do, the hypothesis is always in the last paragraph of the introduction. Where the Research Question sits in the Introduction can vary - some put it near the start to give the reader an early insight; others leave it until much later on as the writing can flow better that way. But the introduction to the methods transition works best when the hypothesis is at the end of the introduction.

### Should we give equal weight to all the variables in the introduction?

That is one option and that can work if you are equally interested in all variables and each are as equally important to what you are overall trying to show. Alternatively,  it might be that one of the variables takes more of the focus in the introduction - perhaps what you are testing has not been shown and one of the variables has had little research on it. However, when there are only two variables there is probably going to be more of an equal balance but doesn't have to be exactly equal. The main thing to try and avoid is that one variable is covered in great depth and the second is just briefly mentioned - that won't create a good rationale for the study.

### Can I change my hypothesis?

In practice, what you would probably do here if running your own full project would depend on whether you had run the analysis or not. As soon as you run the analysis you should always write up what you originally intended and just deal with any differences in the discussion. If you haven't run the analysis you can effectively scrap everything and run a new pre-reg. But yeah, the point of no return, if you like, is running the analysis.

### Can I say my study is a replication?

There is a distinction between direct and conceptual replication. With direct replication you are pretty much doing the exact same test as the original paper. With a conceptual replication it is a bit vaguer - that would be more close to such and such was found to hold in X, and if that reasoning and theory is correct then the same effect should be seen in Y.  So that can be stated. However, when a direct replication becomes a conceptual replication, and when a conceptual replication becomes just a whole new study inspired by something previous is really not clearly defined. Long story short, here you might think of this work as a conceptual replication but it might not be a direct replication given that the sample is likely to change. If you want to read into the distinction between conceptual and direct replications, we recommend [Brandt et al. (2014)](https://www.sciencedirect.com/science/article/pii/S0022103113001819) who provide a checklist of what makes a convincing replication. 

### Should the full report have the same hypothesis as the pre-registration?

If the question is should the full report have the same hypothesis as was stated in the pre-registration then the answer would be most likely. You can deviate or change of course but be sure to do that before you look at the data and definitely not after. At this stage it can be better to just go with the hypothesis you had though and deal with differences in the Discussion.

### Is it an issue if the results are opposite to the hypothesis we stated - e.g. we hypothesised a negative correlation but it turned out to be positive?

This is not an issue and can make for a really interesting discussion. You have found something different from previous studies, assuming you based the hypothesis on previous studies, so you can now think about and discuss why that difference exists. Avoid assuming it is because you have done something wrong or the study was flawed - look for differences that might potentially explain the difference and perhaps build future directions from that. That said, if there were limitations that may have raised an issue then perhaps discuss them as well.

### Can we change the rationale for our effect size or other aspects of what we have proposed? And how might we do this without sounding clunky?

Again this assumes you haven't run the analysis first but there is nothing wrong in saying “Originally we had planned for ….. based on research into……..(citation). However, on reflection, and based on ………(citation) it would be more appropriate to estimate an effect size…….” That is totally fine and totally honest and would be something we would write. Really not about trying to catch people out in research and more just asking people to state what they are doing and why. However, if you have run the analysis then just deal with any issues and differences in the Discussion and don't go back and change part of the analysis to get a better/different finding.

Please note that this is different however from saying "we had planned an effect size of X and that required N participants. We ended up with N+100 participants and so ran the power analysis again to determine the smallest effect size our design could find". This is completely fine and most likely what a lot of people will do here.

### My results weren't significant, am I going to fail?

No. This is never about finding a significant result. It is purely about writing up research regardless of the outcome.

### Do we need to stick to a rigid Discussion format of summarise all findings first then theory?

The common approach would summarise all findings first and then deal with each in turn but that doesn't mean that an alterantive can't work. We think it might save words by taking the standard approach of all findings summarised and then each in turn related to theory and previous work, but that is just a guess and it may be marginal. But yeah, but both options could/would work. Of course, most of you will only have one finding in this study so summarising it and then discussing it in terms of research and theory is the best approach.

### I am not sure what details of previous research to focus on in the Introduction. Any suggestions?

Actually this is likely to change depending on where you are in the Introduction. Look for this the next time you are reading a paper. Towards the start of the introduction you tend to see a much more global summary of the findings of papers and there is not much detail about what happened. Here people are setting the background broadly. As the Introduction progresses you tend to see more and more detail being mentioned about previous studies. Not all the individual details are mentioned however. What is mentioned is a) what is relevant to the current study, and b) what might help with discussion. So what is relevant might be the main overall finding, and what adds to the discussion might be how it was done, on who, and where, and how big was the effect. Say for instance, the rationale for your study is that it has never been tested in postgrads. In your introduction you will want to make it clear that the previous research was in undergrads or school children. For example, "Paterson et al. (2015), testing children between the ages of 10 and 15, found that...." So here we are giving the finding, but we are also giving detail that will help make our rationale more salient when we state it. Also, it gives us room for discussion in the Discussion section. Say you are doing a close replication of a finding but in the end your results are completely different to what was previously found. If you have given some detail as to the previous study - who and how - then you give yourself more room for comparison, as to why there was a difference, later on in your discussion. So to go back to the question, as you go further through the Introduction you tend to pull out more detail but you only need to mention the detail that is relevant to what is being tested.

### If we have space can we not just state all the details of previous research?

Firstly, if you find you have the space to do that then you are probably not covering enough previous research. Secondly, think about the reader and trying to give them pointers as to what is to come.  Compare these two sentences:

* Paterson et al. (2015), in children aged 10-15, found.....
* Paterson et al. (2015), in Glaswegian right-handed children aged 10-15 using a 9-point Likert scale tested online on a Friday, found.....

If the rationale is just about age then the first sentence hits that point clearly without confusing the reader. In contrast, the second sentence makes the reader think they have to hold all this information, only to later find out that the key info was age and the rest isn't really being tested here. So details are interesting but not always relevant. Maybe all the detail is relevant to your study but, if it isn't, you might just be confusing your reader.

### Does it matter the order in which I present research in my introduction.

It used to be that people tried to present the research in order of most relevance to their own study so that the most relevant, and most detailed, was mentioned just before they moved into the last paragraph where they focussed on "In the current study...". This can be a good approach as it fits nicely with the Broad to Narrow idea; you are leading the reader through the introduction and building up their knowledge as you go. This should give a nice logical flow, so that when it gets to the hypothesis and rationale for the current study, it is really clear to the reader.

# Methods Section

### In the Design and Analysis subsection of the Methods do we just say what we did or do we need to justify and cite why we chose certain methods?

In general your Methods section will be mostly descriptive, just stating what you did, who you ran, and how, within the full report. Caveats to that may be things like your inclusion/exclusion criteria where you may wish to provide some justification, or perhaps you want to add a citation to support any discussion of using one approach over another, e.g. using the Welch's over the Students, but if just stating we used a Pearson correlation then there is no need for citation. Overall though the Methods section should be clear and concise. 

### How exactly do we use the power and effect sizes reported in our pre-reg as part of our descriptive and inferential analyses in the Quant Report? 

There is these blogs at the end of the power chapter that might help: [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#additional-information](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#additional-information){target="_blank"} Your pre-reg is an estimate of how many people you needed. The full dataset is how many people you got. With that full dataset you can determine an effect of X. You run the analysis and find an effect of Y. If X is less than Y then your study was sufficiently powered. If X is bigger than Y then your study was underpowered. If your study is sufficiently powered and you find a significant effect then there is some support for it being a reliable effect. If your study is underpowered then no matter what you find, significant or not significant, is a bit unclear.

### Do we need subheadings in the Methods Section?

Yes. This is important as it is part of the APA formatting of a report and using the professional structure. The main subheadings would be Participants, Materials, Procedure, and Design and Data Analysis. Note however that sometimes the information in the Design and Data Analysis subsection could be included in the start of Results section instead. It comes down to what works best for your writing.

### Can we change the order of subheadings in a Methods Section?

The standard is usually Participants, Materials, Procedure, Design and Data Analysis, as this follows the logical flow of who, with what, and how - who did you test, what did you use, and what did they do, and how are you going to analyse it - however, if the information in your writing would make more sense in a different order then that would be acceptable. Do watch out however that logical flow is maintained in the order - do not try to explain things before the reader has the information they need to understand what you are talking about.

### Do we need an Ethics subsection in the Methods?

No but if you wanted to include one that is fine, or to include a sentence in the Procedure maybe. Please note that this is not essential for this report however. The study was granted ethics through the Ethics Committee of the School of Psychology and Neuroscience within the College of Science and Engineering.

### What is the main difference between a Materials subsection and the Procedure subsection?

The main difference is that the Materials details what you used - stimuli, questionnaires, etc - whereas the Procedure details what participants did and how the study was run. For example, "The possible answers on each question was 1 to 5 with 1 meaning....." is Materials. Whereas, "Participants responded by using the mouse to select the appropriate button indicating...." is Procedure. Procedures for questionnaires aren't particularly long but there are some tips below to think about and a good thing to do is run yourself in the study again and think about some of the tips in that context.

### I have seen Cronbach's alpha mentioned in papers. What is this and do we need it?

In terms of a questionnaire, Cronbach's alpha measures a type of reliability known as internal consistency. Normally a Cronbach of $\alpha$ > .8 (so between .8 and 1) is considered very good reliability. When you create a questionnaire, you are trying to measure a type of behaviour across multiple questions. If the questionnaire measures one behaviour, then people should respond in a similar way to all the questions. Therefore, good internal consistency means people respond similarly to all the questions, but poor internal consistency means people respond less similar to all the questions and it may not be measuring one type of behaviour.

You do not need to calculate this for this report as we have not covered it. However you can include it if you like in the materials when talking about the MSLQ if you feel confident in it. Normally it is just a statement of "Previously this scale has been shown to have good internal consistency ($\alpha$ = .XX), (citation)". But this is not essential in this report as we have not covered it.

### Do we need to discuss the code we wrote and how it worked?

No. Again not wrong but it is using up space and is really not needed and not the done thing in reports.  If you find yourself saying "we loaded in the csv file and the filtered it for X and then mutated on a column before creating a mean value for each participant" then rephrase this as simply "a mean value was calculated for each participant" as that is the key bit. You do not need to detail how the code work and you are likely using up words that should be saved for other sections.

### When we calculated power for the t-test in the pre-reg we were expecting equal sizes. How can I calculate the power with unequal sizes?

There is also an example within the the Fundamentals of Quantitative Analysis book here if that helps others: [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#uneven-groups](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#uneven-groups){target="_blank"}. Remember that you want to know the smallest `d` that you can determine with your sample sizes.

### In the participants section should I show the mean age and SD of groups regardless of whether I am looking at age or not?

It would be really recommended. In any research it is important that a reader can contextualise the sample - who were they? There is a big difference between a sample of 60 year olds and a sample of 18 year olds and if you don't tell your reader who was tested then they can never contextualise the results. Most commonly you will see a breakdown of the number of participants and the mean age and standard deviation of age for categorical variable of interest - e.g. Mature Students (N = ..., Mean Age = ..., SD = ....).  You may see other information such as nationality but that additional information is more important when it is relevant and not just standard practice.

### Should I give the total N and Mean Age etc on the sample before exclusions, after exclusions, or both?

The most important one is after exclusions because that is who will ultimately be tested upon. It might be interesting to include information prior to exclusion - as it can sometimes show issues - but that might come down to availability of words. Main thing to think about is what is relevant and to not go into too much detail of what is not relevant using up words.

### I wanted to show a mean age but some didn't give their age and I don't want to filter them out?

In that situation we might do something like "150 participants took part in our study (Mean Age = ..... years, SD = ......; 45 declined to answer)......."  At least that way you are making it clear the age and sd are based on what you have and you are making it clear that X didn't answer.

### What year and how was the data from the MSLQ collected?

In reality it was used predominantly in Level 1 (First year undergraduate class) and MSc labs, and students were asked to complete it as part of their course. They could also send the invite to others meaning that we can't guarantee that everyone was in the University of Glasgow. It was also left open on the main experiment page so others could do it as well. In terms of when it started, there is a datastamp on the datafiles that can indicate when first data was obtained.

### Do we need citations in our methods section?

You would definitely need to cite the MSLQ and Experimentum as these are something someone has created and made available. Beyond that it comes down to the argument being made. More often than not a Methods section is light on citations. Only when you are making an argument for doing something - like not using the Shapiro-Wilks after you said you would use it - or doing something different from the standard - opted to use Kendall's Tau instead of Pearson and/or Spearman for reason X, Y and Z - would you use a citation.

### Do we need to define all terminology, e.g., between-subjects?

No, you can assume a basic level of methodological understanding on behalf of the reader. You should however define your variables in the introduction, for example, what exactly do you mean by test anxiety, what is the official criteria for a “mature” student etc.

### Do we need a citation for stating $\alpha$ and $\beta$ levels?

No, as long as you are using the field standards of $\alpha = .05$ and $\beta = .8$. There you would just say "using the field standards of..." You can of course include a citation if you have one but it isn't really needed. That said, however, if you decide to use values different from the field standards, e.g. $\alpha = .01$ and $\beta = .95$ then you will want to briefly state why and give a citation or two to support this.  

# Results

### What happens if you are underpowered?

Have a read at this blog and see if that helps? [https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#a-blog-on-interpreting-and-writing-up-power](https://psyteachr.github.io/quant-fun-v2/power-and-effect-sizes.html#a-blog-on-interpreting-and-writing-up-power){target="_blank"}. Being underpowered is just something that happens. The trick is to just avoid saying there is absolutely nothing going on here. When underpowered the truth is you just don't know what is actually happening. You know the effect is not this big but you dont know if it is really this small and can't rule it out so you have to be cautious in the write up. "The absence of evidence is not evidence of an absence" is how someone much smarter than us put it once, but it is the trap a lot of seasoned researchers fall into and one we are trying to teach people to avoid. Another way of phrasing it is, "I dont know what it is but I know what it isn't" i.e. I know it isnt a big effect but I dont know if it is a medium or small effect or nothing so I am just going to leave it at that.

### How many decimal places should we use in values?

The APA Style guide webpage has a good guide on this that it is handy to know about: [Numbers and Statistics in APA](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf){target="_blank"}. This is not always adhered to though in publications and often the APA defer to the teachers of a course as the final say. However, if in doubt, using this guideline is totally fine.  One common approach people take is three decimal places for p-values and correlation values (r-values), and two decimal places for everything else. Really the hidden rule behind this approach is whether the value can go above 1 or not. p-values and correlation values (Pearson r) can't go above 1 and so we drop the first 0 and go to three decimal places - e.g. r = .015 and not r = 0.015 or r = 0.02. Often as well people might put Cohen's d to three decimal places, even though it can go above 1, especially when it is small - e.g. d = 0.005 rather than d = 0.01 as rounding up really changes the effect size. So there are always a few caveats and nuances. The main advice is to try and be consistent at this level, and do not use more than three decimal places.

### My degrees of freedom for my Welch's t-test has a decimal place. Is that ok and should I round it up?

When you run a Welch's t-test the degrees of freedom is likely to have a decimal place and should not be rounded up. Convention is to state the Welch's degrees of freedom to two decimal places as it can go above 1. For example, t(95.56) = -0.91, p = 0.367, d = -0.19.

### If my t-test is negative have I done something wrong?

No. Remember that the polarity of a t-test (whether it is positive or negative) just helps determine which group was bigger. More often than not, when t-tests are presented in research they are presented as positive (or really the absolute value - without polarity) but it is not wrong to present the t-test with a negative t-value. Do remember though that the t-value and the effect size (cohen's d) should have the same polarity: either both positive or both negative.

### My t-test confidence interval is showing funny? It says Inf?

That is fine if you have run a one-tailed t-test. It can be written as 95%CI = [0.61, Inf] assuming your effect is positive. If it was negative then -Inf is fine. It is because it is a one-sided test and the confidence estimate of the upper bound can't be estimated properly anymore (or that it could possibly be anything; we can't quite recall the distinction right now).

### Should figures and tables in the Results section be in APA format?

Preferably if you can but as we have not actually covered this yet it is not essential and won't be considered wrong if you do not use APA formatting.  You can get a good idea of APA formatting for figures and tables by looking at the guidelines here in the APA style blog: [https://apastyle.apa.org/style-grammar-guidelines/tables-figures](https://apastyle.apa.org/style-grammar-guidelines/tables-figures){target="_blank"}. More specifically the tabs titled "Sample Figure" and "Sample Table". There is also a really nice paper and guide from the team here at Glasgow on creating figures - [https://psyteachr.github.io/introdataviz/](https://psyteachr.github.io/introdataviz/) as well as a chapter in your Data Skills book - [https://psyteachr.github.io/quant-fun-v2/visualisation.html](https://psyteachr.github.io/quant-fun-v2/visualisation.html).

### How can I do an APA figure in R?

Again not required for this report but if you were to add `+ theme_classic()` to the end of a ggplot chain then you get something close enough. An example is below. But again this is not required in this report. 

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

ggplot(iris, aes(Sepal.Length,Sepal.Width)) + 
  geom_point() +
  theme_classic()

```

### How can I do an APA table in R?

Honestly it is not worth trying to do it in R right now. Best thing is to create the table in R so you have the values and then just create a table in Word and paste the values in.  You can actually get Word to give you a template of an APA table as a starter though if that would help. If you open a new word doc, but instead of just clicking "blank document", in the search online templates box type "APA", you will see the APA template. Once you select that and it opens, scroll down to page 7 and you will see the template table. You can then copy and paste that into your doc in the Results section and then just edit that as you need.

### Can I use stars in my table to show different levels of significance? For example, p < .05 would be one *?

On the * vs ** thing. So this is used in the literature but some are a fan and some aren't. We would say not to use them but that is maybe a conversation for another day and doesn't really have any bearing on this report. Just flagging that at times you will get into a conversation of whether this approach is good or not so good.

### I don't have any tables in my Results section or Methods section, either for descriptives or for correlations, is that ok?

Yes. If you only compare two groups or only run one correlation, often that information can be conveyed quite effectively just using text and not using a table - keeping in mind tables count in the word count and require legends. Some will have a table because it helps them. Some will not have a table because it doesn't help them. Both are fine.

### Should I show all my assumptions figures in the Results section?

No. It isn't needed. Again it isn't wrong to do so but it is taking up space as each figure needs a legend and needs talked about.

### How many figures should be in my Results section?

Normally there is one figure per analysis. So one t-test gives you one boxplot (or equivalent) and one correlation gives you one scatterplot. If you do one t-test or one correlation in your study then you would likely have one figures in your Results section.

### Can I do more than just the minimum analysis?

Ultimately we will mark and give feedback on what you submit. That means that if you were inspired to run a series of correlations then we would still mark that. However, please note that this report is not about showing how many analyses you can do. It is about showing how you can write up the research analysis. The more analyses you do, the more space you take up from writing your introduction and discussion, and remember that every finding would need discussed in the Discussion. So whilst you can do more analyses, it is not advised.

### Is it possible to compare different effect sizes such as Pearson's r, Cohen's d, partial eta-squared, and Hedges' g?

Yes but the best thing to do is convert them all into the same effect size first so that you are comparing like for like. Often if you google "convert Cohen's d to...." you might find what you need and then you can compare across studies. If you can't convert the given effect size into a different one for some reason - not enough information for example - then you can compare them semantically "small vs large" but it would be worth acknowledging that they were different effect sizes in your writing somehow.

### My data looks normal but the Shapiro-Wilks is showing as significant which would suggest not normal. Can I ignore my Shapiro-Wilks?

This is likely because you have a large sample size. Here we would advise disregarding the Shapiro-Wilks but stating why. This paper [https://link.springer.com/article/10.3758/s13428-021-01587-5](https://link.springer.com/article/10.3758/s13428-021-01587-5){target="_blank"} has a useful point in it: "Formal tests for normality have been criticized because they have low power at small sample sizes and almost always yield significant deviations from normality at large sample sizes (Ghasemi & Zahediasl, 2012)."

### Do we need to justify and give citations for each action and analysis in the Methods and Results?

This aspect can be more brief in the full report as compared to the pre-registration so aim to provide brief justifications where needed. You would not need to citation the tests themselves e.g. a citation for a Pearson's correlation is not needed. Citations are more for when you are stating a reason for doing something that is perhaps a little different from the current norm - such as "We are using Welch's as it is better at controlling False Positive (citation)" as opposed to "We are using Pearsons because all assumptions were met".

### My test was significant but the effect size/relationship was really really small. Have I done something wrong?

Probably not. When you have a large sample, small effects/relationships will be significant. Check your code and check your degrees of freedom make sense. If all that seems correct then your result is probably correct just that you have a very significant and very small effect. This is why we should be more interested in effect sizes than p-values.

### You said something about checking degrees of freedom to check my results make sense. What does that mean?

When working with big data - or any data really - it can be easy to make a mistake. One thing you do know though is how to calculate the degrees of freedom from your test. In a correlation it is N - 2 for example. So if you have 100 participants then your degrees of freedom should be 98. If you run a correlation with 100 people and the degrees of freedom (parameter) is not 98, something is wrong.  Likewise with a t-test but it is a bit harder because you are likely to run a Welch's t-test so the degrees of freedom is harder to calculate. However, if you briefly run it as a Student's between-subjects t-test (by setting `var.equal = TRUE`), check the degrees of freedom makes sense, then switch it back to a Welch's t-test, you can be pretty sure your Welch's output is correct. If the degrees of freedom didn't make sense as a Student's test then something is wrong.

## We are doing a correlation, do we need to show the means and standard deviations of the variables?

Try to stop focusing on "need to" and think more about what is informative. The reason that many state the means and standard deviations on continuous variables, even though it is the overall relationship being analysed and not the difference between groups, is because it helps contextualise the sample. Say you are looking at test anxiety and find a relationship with peer support and conclude that people with higher test anxiety use peer support more. That is informative, but if you have not give the means and standard deviations of the test anxiety then a reader has no idea if your sample was all spread out or really skewed to people who all scored low overall. Giving that additional information helps the reader understand what is going on, and may give you some more information to focus on in the discussion.

# Citing R

### Should we cite R, RStudio and packages used?

The minimum should be citing R and packages used. You can cite RStudio as well but the thing to note is that you should not just cite RStudio by itself - again citing R and the packages is the minimum. There is an additional page at the back of the Data Skills book that gives some help on this: [https://psyteachr.github.io/quant-fun-v2/citing-r-rstudio.html](https://psyteachr.github.io/quant-fun-v2/citing-r-rstudio.html)

### How do you cite R and packages?

First off, you should really cite R and the packages you use. Someone has spent a lot of time doing the work to put these packages etc together so we can do our work effectively and efficiently so they should really get the credit. And to us, it is just like citing a paper. So yeah, in a report you really should cite the software you use.

A regular mistake that we see is people citing RStudio as the software. This is wrong because RStudio is just an Integrated Development Environment (IDE) for using R – with R being the main programme we use. If you like, RStudio is just a way of making R look nice - plus some additional functionality for making R easier to work with. Citing RStudio is a bit like citing the make of your TV when really you watched something on the BBC (or other local channel). What TV you watch something on may change how it looks a bit but it is not doing all the hard work on journalism really. The main thing to recognise however is that it is R you are learning practical skills in, and it is R that you are using to analyse and wrangle data, not RStudio (it's just a window onto R), and as such when we are writing up our work we should cite R. 

So how do we do that? Here is a sentence that you might see in journal articles for an analysis conducted in R using the tidyverse and pwr:

"All data wrangling, visualisation and analysis was conducted using the R programming environment (Version 4.0.2, R Core Team, 2020) using the tidyverse library (Version 1.3.0, Wickham et al., 2019) and pwr package (Version 1.3.0, Champely, 2020)."

You may also see this:

"All data wrangling, visualisation and analysis was conducted using the R programming environment (Version 4.0.2, R Core Team, 2020) using the tidyverse library (Wickham et al., 2019) and pwr package (Champely, 2020)."

And just to note, the difference in that second one is the package version has been dropped off of the packages. For this report, it is totally acceptable to have the package versions included or not. I would say though to include the Version of R as that generally makes a bigger difference.  I can get into why I would and wouldn’t have the package version in a full paper later if people are interested (word count, code available, references) but again I would say here in this report, R Version is good and package versions can be added or not. Please note though that the citation is always there. 

You may also see:

"All data wrangling, visualisation and analysis was conducted using the R programming environment (Version 4.0.2, R Core Team, 2020) using the tidyverse library (Wickham et al., 2019)”.

And then somewhere later on in the paper

“Using the pwr package (Champely, 2020) we established….."

Showing that the information can be spread throughout and I am just using the citations like a paper citation. I.e. all the packages don’t have to be in one long list and you don’t have to mention R ten different times if you split it up.

Of course if you haven't done certain parts in R, you would state what you did them in (excel for instance), but if you did it all in R then the above would be appropriate, assuming you had only used Version 4.0.2 of R, Version 1.3.0 of the tidyverse, and Version 1.3.0 of the pwr package. And then in the reference sections you would include something like this below (or equivalent APA 7th version):

Champely, S. (2020). pwr: Basic functions for power analysis (Version 1.3.0)

R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

One thing to add though on the references of packages. It isn’t always clear what information you are actually meant to include, and can be difficult to make sense of, so do not worry about this too much. Just do your best with it and I would say as a minimum in the references have the last name, first initial, year, package name if possible.

Oh, and before I forget, as you will remember, tidyverse is actually a collection of packages (ggplot2, dplyr, etc, see: https://www.tidyverse.org/packages/) and there is some debate whether to state each package individually or just state the tidyverse library. For this report I would say both options are fine and it is totally acceptable for this report to just state tidyverse library and not all the individual packages contained within tidyverse.

Hopefully that makes some sense. I think the main parts you will probably need to check are what version of R you are using and citations for packages. This is fairly straightforward using the at the back of the Data Skills book: [https://psyteachr.github.io/quant-fun-v2/citing-r-rstudio.html](https://psyteachr.github.io/quant-fun-v2/citing-r-rstudio.html)

So hopefully that helps. Main take-away thing to remember is that you should cite R and relevant packages. You can cite RStudio in addition if you like.

### Do we need to cite all packages contained within tidyverse?

There is debate on this as mentioned above. We would recommend that for this report it is fine to just cite the tidyverse but going forward it would be better to cite all packages.